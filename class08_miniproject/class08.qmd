---
title: "class 8: Breast Cancer Mini Project"
author: "Kai Zhao (PID: A17599942)"
format: pdf
---

## Outline
Today we will apply the machine learning methods we introduced in the last class on breast. cancer biopsy data from fine needle aspirations (FNA)

##Data input
The data is suppied on CSV format:

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names = 1)
head(wisc.df)
```
Now I will store the diagnosis for later and exclude it from the data set I will actually do things wuth that I will call `wisc.data`
```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
wisc.data <- wisc.df[,-1]
```

>Q1 How many people are in this data set?

```{r}
nrow(wisc.df)
```

>Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```
```{r}
sum(wisc.df$diagnosis=="M")
```


>Q3. How many variables/features in the data are suffixed with _mean?

```{r}
x <- colnames(wisc.df)
length(grep("_mean",x))
```
```{r}
x 
```


```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)

# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, center = TRUE, scale. = TRUE)
# Look at summary of results
summary(wisc.pr)

```

>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

```{r}
pc1_variance <- summary(wisc.pr)$importance[2, 1]
pc1_variance
```
>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

```{r}
cum_var <- cumsum(summary(wisc.pr)$importance[2,])
num_pcs_70 <- which(cum_var >= 0.70)[1]
num_pcs_70
```
>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

```{r}
num_pcs_90 <- which(cum_var >= 0.90)[1]
num_pcs_90
```




# Principal Component Analysis

We need to scale our input data before PCA as some of the columns are measured in terms of very different units with different means and different variances. The upshot here is we set `scale=TRUE` argument to `prcomp()`

```{r}
wisc.pr <- prcomp( wisc.data, scale= TRUE )
summary (wisc.pr)
```
Generate one of our main result figures - the PC plot (a.k.a. "source plot", "orientation plot", "PC1 vs PC2 plot", "PC plot","Projection plot", etc.) It is known by different names in different fields.

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(wisc.pr)

```

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,2], col =diagnosis)
```
>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?


```{r}
# Scatter plot of PC1 vs PC3 colored by diagnosis
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis,
     xlab = "PC1", ylab = "PC3")

```

Add a ggplot version

```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

# Calculate variance of each principal component
```{r}
pr.var <- (wisc.pr$sdev^2)
head(pr.var)
```
# Variance explained by each principal component: pve

```{r}
pve <- pr.var / sum(pr.var)
```


# Plot variance explained for each principal component
```{r}

plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

```
# Alternative scree plot of the same data, note data driven y-axis

```{r}
barplot(pve, ylab = "Percent of Variance Explained",
        names.arg = paste0("PC", 1:length(pve)), las = 2, axes = FALSE)
axis(2, at = seq(0, max(pve), by = 0.05), labels = round(seq(0, max(pve), by = 0.05) * 100))

```
>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean", 1]
```

>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
cumulative_pve <- cumsum(pve)
which(cumulative_pve >= 0.8)[1]

```


# Hierarchical clustering

Can we just use clustering on the orginal data and get some insight into M vs B?

It is difficult, this "tree" looks like a hot mess...


>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
#distance matrix needed for hclust

data.dist <-  dist(scale(wisc.data))
wisc.hclust <-  hclust(data.dist)
plot(wisc.hclust)
abline(h = wisc.hclust$height[length(wisc.hclust$height) - 4 + 1], col="red", lty=2)
```


```{r}

wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)

```
>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
comparison_list <- list()

for(k in 2:10) {
  clusters <- cutree(wisc.hclust, k)
  comp_table <- table(clusters, diagnosis)
  comparison_list[[paste0("k=", k)]] <- comp_table
}

comparison_list

```
>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

```{r}
methods <- c("single", "complete", "average", "ward.D2")

results = list()

for(method in methods) {
  hclust_obj = hclust(data.dist, method = method)
  clusters = cutree(hclust_obj, k = 4)
  comp_table = table(clusters, diagnosis)
  results[[method]] = comp_table
}

results

```




## 5. combining methods

This approach will take not orginal data but our PCA results and work with the.



```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method = "ward.D2")
plot(wisc.pr.hclust)
```

Generate 2 cluster groups from this hclust object.

```{r}
grps <- cutree(wisc.pr.hclust, k= 2)
grps
```

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,2],col=grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```


```{r}
table(grps)
```

```{r}
table(diagnosis)
```


```{r}
table(diagnosis, grps)
```
>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
d <- dist(wisc.pr$x[,1:7])
wisc.pr.hclust <- hclust(d, method = "ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table( wisc.pr.hclust.clusters , diagnosis)
```

